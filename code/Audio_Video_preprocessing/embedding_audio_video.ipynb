{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alp8iUzdruOZ"
      },
      "source": [
        "# Description\n",
        "1. BEiT3 모델을 초기화하여 이미지 임베딩을 추출합니다.\n",
        "2. HuBERT 모델을 사용하여 오디오 임베딩을 추출합니다.\n",
        "3. 두 모델을 활용하여 비디오에서 이미지, 오디오 임베딩 정보를 추출합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw7RWAS7ppGR",
        "outputId": "07d39635-da0c-446c-b14c-593e340483f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchscale in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from torchscale) (2.6.0+cu124)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.11/dist-packages (from torchscale) (0.4.0)\n",
            "Requirement already satisfied: timm==0.6.13 in /usr/local/lib/python3.11/dist-packages (from torchscale) (0.6.13)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.6.13->torchscale) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm==0.6.13->torchscale) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from timm==0.6.13->torchscale) (0.28.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->torchscale) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->torchscale) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->torchscale) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchscale moviepy torchaudio transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N9rM37dsrHl"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4CReBNh25b_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "import moviepy.editor as mpy\n",
        "from transformers import Wav2Vec2FeatureExtractor, HubertModel\n",
        "from torchscale.architecture.config import EncoderConfig\n",
        "from torchscale.model.BEiT3 import BEiT3\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtM6qQlost_o"
      },
      "source": [
        "# Class, Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mftt401z3mDv"
      },
      "outputs": [],
      "source": [
        "class AudioEncoder:\n",
        "    \"\"\"HuBERT 모델을 사용한 오디오 인코더 클래스\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"facebook/hubert-base-ls960\"):\n",
        "        \"\"\"\n",
        "        HuBERT 모델을 사용하여 오디오 인코딩을 위한 클래스 초기화\n",
        "\n",
        "        Args:\n",
        "            model_name (str): 사용할 HuBERT 모델 이름\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"오디오 인코더 장치 사용: {self.device}\")\n",
        "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "        self.model = HubertModel.from_pretrained(model_name).to(self.device)\n",
        "        self.sample_rate = 16000  # HuBERT는 16kHz에서 훈련됨\n",
        "\n",
        "    def extract_audio_from_clip(self, clip):\n",
        "        \"\"\"\n",
        "        moviepy 클립에서 오디오를 추출하여 임시 파일로 저장합니다.\n",
        "\n",
        "        Args:\n",
        "            clip (VideoClip): moviepy 비디오 클립\n",
        "\n",
        "        Returns:\n",
        "            str: 추출된 오디오 파일 경로\n",
        "        \"\"\"\n",
        "        # 임시 파일 생성\n",
        "        temp_dir = tempfile.gettempdir()\n",
        "        audio_path = os.path.join(temp_dir, 'extracted_audio.wav')\n",
        "\n",
        "        # 오디오 추출 및 WAV 파일로 저장 (16kHz, 모노)\n",
        "        clip.audio.write_audiofile(audio_path, fps=self.sample_rate, nbytes=2, codec='pcm_s16le', verbose=False, logger=None)\n",
        "\n",
        "        return audio_path\n",
        "\n",
        "    def preprocess_audio(self, audio_path):\n",
        "        \"\"\"\n",
        "        오디오 파일을 로드하고 전처리합니다.\n",
        "\n",
        "        Args:\n",
        "            audio_path (str): 오디오 파일 경로\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: 전처리된 오디오 텐서\n",
        "        \"\"\"\n",
        "        # 오디오 파일 로드\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # 모노로 변환 (스테레오인 경우)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # 샘플 레이트 재조정 (필요한 경우)\n",
        "        if sample_rate != self.sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        return waveform.squeeze()\n",
        "\n",
        "    def encode_segment(self, segment):\n",
        "        \"\"\"\n",
        "        단일 오디오 세그먼트를 인코딩합니다.\n",
        "\n",
        "        Args:\n",
        "            segment (torch.Tensor): 오디오 세그먼트\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: 인코딩된 특성 벡터\n",
        "        \"\"\"\n",
        "        # 특성 추출기를 사용하여 입력 준비\n",
        "        inputs = self.feature_extractor(\n",
        "            segment,\n",
        "            sampling_rate=self.sample_rate,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # 모델 추론 (gradient 계산 없이)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # 출력의 평균을 계산\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        return embeddings.cpu()\n",
        "\n",
        "    def encode_audio_clip(self, clip):\n",
        "        \"\"\"\n",
        "        moviepy 오디오 클립을 인코딩합니다.\n",
        "\n",
        "        Args:\n",
        "            clip (VideoClip): moviepy 비디오 클립\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: 인코딩된 오디오 임베딩\n",
        "        \"\"\"\n",
        "        if clip.audio is None:\n",
        "            raise ValueError(\"비디오 클립에 오디오 트랙이 없습니다.\")\n",
        "\n",
        "        # 임시 오디오 파일로 저장\n",
        "        audio_path = self.extract_audio_from_clip(clip)\n",
        "\n",
        "        try:\n",
        "            # 오디오 전처리\n",
        "            waveform = self.preprocess_audio(audio_path)\n",
        "\n",
        "            # 오디오 인코딩\n",
        "            audio_embedding = self.encode_segment(waveform)\n",
        "\n",
        "            return audio_embedding\n",
        "        finally:\n",
        "            # 임시 파일 삭제\n",
        "            if os.path.exists(audio_path):\n",
        "                os.remove(audio_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuuKS_Kj3m4f"
      },
      "outputs": [],
      "source": [
        "class BEiT3ForEmbedding(nn.Module):\n",
        "    \"\"\"BEiT3 모델을 사용한 이미지 임베딩 추출 클래스\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def initialize(self, checkpoint_path, model_type='base', input_size=224, device=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            checkpoint_path: 체크포인트 파일 경로\n",
        "            model_type: 'base' 또는 'large'\n",
        "            input_size: 입력 이미지 크기\n",
        "            device: 사용할 디바이스 (None이면 자동 선택)\n",
        "        \"\"\"\n",
        "        # 디바이스 설정\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"비디오 인코더 장치 사용: {self.device}\")\n",
        "\n",
        "        # 이미지 전처리 설정\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # 모델 설정\n",
        "        if model_type == 'base':\n",
        "            config = self._get_base_config(input_size)\n",
        "        else:\n",
        "            config = self._get_large_config(input_size)\n",
        "\n",
        "        # BEiT3 모델 초기화\n",
        "        self.beit3 = BEiT3(config)\n",
        "\n",
        "        # 레이어 정규화 추가 (패치 임베딩용)\n",
        "        self.fc_norm = nn.LayerNorm(config.encoder_embed_dim)\n",
        "\n",
        "        # 체크포인트 로드\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "        # 체크포인트 구조 확인\n",
        "        if 'model' in checkpoint:\n",
        "            checkpoint_model = checkpoint['model']\n",
        "        else:\n",
        "            checkpoint_model = checkpoint\n",
        "\n",
        "        # 모델 가중치 로드\n",
        "        missing_keys, unexpected_keys = self._load_state_dict(checkpoint_model)\n",
        "        print(f\"BEiT3 가중치 로드 - Missing keys: {len(missing_keys)}, Unexpected keys: {len(unexpected_keys)}\")\n",
        "\n",
        "        # 평가 모드로 설정하고 디바이스로 이동\n",
        "        self.eval()\n",
        "        self.to(self.device)\n",
        "\n",
        "        print(f\"BEiT3 ({model_type}) 임베딩 모델 초기화 완료\")\n",
        "        return self\n",
        "\n",
        "    def _get_base_config(self, img_size=224, patch_size=16, vocab_size=64010):\n",
        "        \"\"\"BEiT3 base 모델 설정\"\"\"\n",
        "        return EncoderConfig(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            vocab_size=vocab_size,\n",
        "            multiway=True,\n",
        "            layernorm_embedding=False,\n",
        "            normalize_output=True,\n",
        "            no_output_layer=True,\n",
        "            drop_path_rate=0.1,\n",
        "            encoder_embed_dim=768,\n",
        "            encoder_attention_heads=12,\n",
        "            encoder_ffn_embed_dim=3072,\n",
        "            encoder_layers=12\n",
        "        )\n",
        "\n",
        "    def _get_large_config(self, img_size=224, patch_size=16, vocab_size=64010):\n",
        "        \"\"\"BEiT3 large 모델 설정\"\"\"\n",
        "        return EncoderConfig(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            vocab_size=vocab_size,\n",
        "            multiway=True,\n",
        "            layernorm_embedding=False,\n",
        "            normalize_output=True,\n",
        "            no_output_layer=True,\n",
        "            drop_path_rate=0.1,\n",
        "            encoder_embed_dim=1024,\n",
        "            encoder_attention_heads=16,\n",
        "            encoder_ffn_embed_dim=4096,\n",
        "            encoder_layers=24\n",
        "        )\n",
        "\n",
        "    def _load_state_dict(self, state_dict):\n",
        "        \"\"\"모델 가중치 로드 (원래 BEiT3 포맷에서 일부 수정)\"\"\"\n",
        "        model_state_dict = self.state_dict()\n",
        "\n",
        "        # 모델 가중치 로드\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "\n",
        "        # state_dict의 키 중에서 model_state_dict의 키와 일치하는 것만 로드\n",
        "        for k, v in state_dict.items():\n",
        "            if k in model_state_dict:\n",
        "                if model_state_dict[k].shape != v.shape:\n",
        "                    print(f\"Shape mismatch: {k}, model: {model_state_dict[k].shape}, checkpoint: {v.shape}\")\n",
        "                    continue\n",
        "                model_state_dict[k].copy_(v)\n",
        "            else:\n",
        "                unexpected_keys.append(k)\n",
        "\n",
        "        # model_state_dict의 키 중에서 state_dict에 없는 것은 missing_keys에 추가\n",
        "        for k in model_state_dict:\n",
        "            if k not in state_dict:\n",
        "                missing_keys.append(k)\n",
        "\n",
        "        self.load_state_dict(model_state_dict)\n",
        "        return missing_keys, unexpected_keys\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        이미지에서 임베딩 추출\n",
        "\n",
        "        Args:\n",
        "            images: 이미지 텐서 [B, C, H, W]\n",
        "\n",
        "        Returns:\n",
        "            dict: 다양한 임베딩이 포함된 딕셔너리\n",
        "        \"\"\"\n",
        "        outputs = self.beit3(textual_tokens=None, visual_tokens=images)\n",
        "        x = outputs[\"encoder_out\"]\n",
        "\n",
        "        # CLS 토큰 (첫 번째 토큰)\n",
        "        cls_embedding = x[:, 0, :]\n",
        "\n",
        "        # 패치 토큰들 (CLS 토큰 제외)\n",
        "        patch_tokens = x[:, 1:, :]\n",
        "\n",
        "        # 평균 풀링 임베딩\n",
        "        avg_embedding = self.fc_norm(patch_tokens.mean(1))\n",
        "\n",
        "        return {\n",
        "            'cls_embedding': cls_embedding,         # CLS 토큰 임베딩\n",
        "            'avg_embedding': avg_embedding,         # 평균 풀링 임베딩\n",
        "            'all_tokens': x,                        # 모든 토큰 임베딩\n",
        "            'patch_tokens': patch_tokens            # 패치 토큰 임베딩 (CLS 제외)\n",
        "        }\n",
        "\n",
        "    def get_embedding_from_pil(self, pil_image):\n",
        "        \"\"\"\n",
        "        PIL 이미지에서 임베딩 추출\n",
        "\n",
        "        Args:\n",
        "            pil_image: PIL 이미지 객체\n",
        "\n",
        "        Returns:\n",
        "            dict: 다양한 임베딩이 포함된 딕셔너리\n",
        "        \"\"\"\n",
        "        # RGB 모드로 변환\n",
        "        if pil_image.mode != 'RGB':\n",
        "            pil_image = pil_image.convert('RGB')\n",
        "\n",
        "        # 이미지 전처리\n",
        "        image_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # 임베딩 추출\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.forward(image_tensor)\n",
        "\n",
        "        # CPU로 변환하여 반환\n",
        "        for key in embeddings:\n",
        "            embeddings[key] = embeddings[key].cpu()\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def encode_image_frames(self, frames):\n",
        "        \"\"\"\n",
        "        프레임 리스트에서 비디오 임베딩을 추출합니다.\n",
        "\n",
        "        Args:\n",
        "            frames (list): PIL 이미지 객체 리스트\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: 프레임 임베딩의 평균 텐서\n",
        "        \"\"\"\n",
        "        if not frames:\n",
        "            raise ValueError(\"프레임 리스트가 비어있습니다.\")\n",
        "\n",
        "        frame_embeddings = []\n",
        "        for frame in frames:\n",
        "            emb = self.get_embedding_from_pil(frame)\n",
        "            frame_embeddings.append(emb['cls_embedding'])  # CLS 토큰 사용\n",
        "\n",
        "        # 프레임 임베딩 평균 계산\n",
        "        video_embedding = torch.cat(frame_embeddings).mean(dim=0)\n",
        "        return video_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W41LTlXW3phI"
      },
      "outputs": [],
      "source": [
        "class VideoAudioEncoder:\n",
        "    \"\"\"비디오와 오디오 임베딩을 함께 추출하는 통합 인코더 클래스\"\"\"\n",
        "\n",
        "    def __init__(self, beit3_checkpoint_path, beit3_model_type='base', hubert_model_name=\"facebook/hubert-base-ls960\", input_size=224):\n",
        "        \"\"\"\n",
        "        통합 인코더 초기화\n",
        "\n",
        "        Args:\n",
        "            beit3_checkpoint_path (str): BEiT3 체크포인트 파일 경로\n",
        "            beit3_model_type (str): BEiT3 모델 타입 ('base' 또는 'large')\n",
        "            hubert_model_name (str): HuBERT 모델 이름\n",
        "            input_size (int): 이미지 입력 크기 (224 또는 384)\n",
        "        \"\"\"\n",
        "        print(\"통합 비디오-오디오 인코더 초기화 중...\")\n",
        "\n",
        "        # 비디오 인코더 초기화\n",
        "        self.video_encoder = BEiT3ForEmbedding().initialize(\n",
        "            checkpoint_path=beit3_checkpoint_path,\n",
        "            model_type=beit3_model_type,\n",
        "            input_size=input_size\n",
        "        )\n",
        "\n",
        "        # 오디오 인코더 초기화\n",
        "        self.audio_encoder = AudioEncoder(model_name=hubert_model_name)\n",
        "\n",
        "        print(\"통합 인코더 초기화 완료\")\n",
        "\n",
        "    def extract_frames_from_clip(self, clip, fps=1):\n",
        "        \"\"\"\n",
        "        moviepy 클립에서 지정한 fps로 프레임을 추출하여 PIL Image 리스트를 반환합니다.\n",
        "\n",
        "        Args:\n",
        "            clip (VideoClip): moviepy 비디오 클립\n",
        "            fps (int): 초당 추출할 프레임 수\n",
        "\n",
        "        Returns:\n",
        "            list: PIL Image 리스트\n",
        "        \"\"\"\n",
        "        frames = []\n",
        "        for frame in clip.iter_frames(fps=fps, dtype=\"uint8\"):\n",
        "            image = Image.fromarray(frame)  # numpy array -> PIL Image\n",
        "            frames.append(image)\n",
        "        return frames\n",
        "\n",
        "    def encode_video_segment(self, video_path, start_time=None, end_time=None, fps=1):\n",
        "        \"\"\"\n",
        "        비디오 파일의 특정 세그먼트에서 비디오와 오디오 임베딩을 추출합니다.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): 비디오 파일 경로\n",
        "            start_time (float, optional): 시작 시간(초). None이면 처음부터.\n",
        "            end_time (float, optional): 종료 시간(초). None이면 끝까지.\n",
        "            fps (int): 초당 추출할 프레임 수\n",
        "\n",
        "        Returns:\n",
        "            tuple: (비디오 임베딩 텐서, 오디오 임베딩 텐서)\n",
        "        \"\"\"\n",
        "        # 비디오 로드\n",
        "        video = mpy.VideoFileClip(video_path)\n",
        "\n",
        "        # 세그먼트 추출 (전체 비디오 또는 지정된 부분)\n",
        "        if start_time is not None or end_time is not None:\n",
        "            start = 0 if start_time is None else start_time\n",
        "            end = video.duration if end_time is None else end_time\n",
        "            clip = video.subclip(start, end)\n",
        "        else:\n",
        "            clip = video\n",
        "\n",
        "        try:\n",
        "            # 비디오 임베딩\n",
        "            frames = self.extract_frames_from_clip(clip, fps=fps)\n",
        "            video_embedding = self.video_encoder.encode_image_frames(frames)\n",
        "\n",
        "            # 오디오 임베딩\n",
        "            audio_embedding = self.audio_encoder.encode_audio_clip(clip)\n",
        "\n",
        "            return video_embedding, audio_embedding.squeeze()\n",
        "        finally:\n",
        "            # 자원 해제\n",
        "            video.close()\n",
        "\n",
        "    def encode_video_by_segments(self, video_path, segment_duration=5, fps=1):\n",
        "        \"\"\"\n",
        "        비디오를 일정 길이의 세그먼트로 나누어 각 세그먼트의 임베딩을 추출합니다.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): 비디오 파일 경로\n",
        "            segment_duration (int): 세그먼트 길이(초)\n",
        "            fps (int): 초당 추출할 프레임 수\n",
        "\n",
        "        Returns:\n",
        "            tuple: (비디오 임베딩 텐서 리스트, 오디오 임베딩 텐서 리스트)\n",
        "        \"\"\"\n",
        "        # 비디오 로드\n",
        "        video = mpy.VideoFileClip(video_path)\n",
        "        duration = video.duration\n",
        "\n",
        "        video_embeddings = []\n",
        "        audio_embeddings = []\n",
        "\n",
        "        try:\n",
        "            # 세그먼트별 처리\n",
        "            for start in range(0, int(duration), segment_duration):\n",
        "                end = min(start + segment_duration, duration)\n",
        "                print(f\"세그먼트 {start}~{end}초 처리 중...\")\n",
        "\n",
        "                # 세그먼트 추출\n",
        "                subclip = video.subclip(start, end)\n",
        "\n",
        "                # 비디오 임베딩\n",
        "                frames = self.extract_frames_from_clip(subclip, fps=fps)\n",
        "                if frames:\n",
        "                    video_emb = self.video_encoder.encode_image_frames(frames)\n",
        "                    video_embeddings.append(video_emb)\n",
        "\n",
        "                # 오디오 임베딩\n",
        "                if subclip.audio is not None:\n",
        "                    audio_emb = self.audio_encoder.encode_audio_clip(subclip)\n",
        "                    audio_embeddings.append(audio_emb.squeeze())\n",
        "\n",
        "            return video_embeddings, audio_embeddings\n",
        "\n",
        "        finally:\n",
        "            # 자원 해제\n",
        "            video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AMasPxQsxFJ"
      },
      "source": [
        "# Run\n",
        "\n",
        "```\n",
        "## 경로 설정\n",
        "beit3_checkpoint = \"path/to/beit3_base_patch16_224.pth\"\n",
        "video_path = \"path/to/video.mp4\"\n",
        "\n",
        "## 1. 인코더 초기화 (한 번만 실행)\n",
        "encoder = VideoAudioEncoder(\n",
        "    beit3_checkpoint_path=beit3_checkpoint,\n",
        "    beit3_model_type='base',\n",
        "    hubert_model_name='facebook/hubert-base-ls960',\n",
        "    input_size=224  # 224 또는 384 (체크포인트에 맞게 설정)\n",
        ")\n",
        "\n",
        "## 2. 전체 비디오의 단일 임베딩 추출\n",
        "video_emb, audio_emb = encoder.encode_video_segment(video_path)\n",
        "print(f\"전체 비디오 임베딩 크기: {video_emb.shape}\")\n",
        "print(f\"전체 오디오 임베딩 크기: {audio_emb.shape}\")\n",
        "\n",
        "## 3. 비디오를 세그먼트로 나누어 임베딩 추출\n",
        "video_embs, audio_embs = encoder.encode_video_by_segments(video_path, segment_duration=5, fps=1)\n",
        "print(f\"세그먼트 수: {len(video_embs)}\")\n",
        "\n",
        "## 4. 세그먼트 임베딩을 텐서로 변환\n",
        "video_embs_tensor = torch.stack(video_embs)\n",
        "audio_embs_tensor = torch.stack(audio_embs)\n",
        "print(f\"비디오 임베딩 텐서 크기: {video_embs_tensor.shape}\")\n",
        "print(f\"오디오 임베딩 텐서 크기: {audio_embs_tensor.shape}\")\n",
        "\n",
        "## 5. 특정 구간만 임베딩 추출\n",
        "segment_video_emb, segment_audio_emb = encoder.encode_video_segment(\n",
        "    video_path,\n",
        "    start_time=10,\n",
        "    end_time=15,\n",
        "    fps=1\n",
        ")\n",
        "print(f\"지정 구간 비디오 임베딩 크기: {segment_video_emb.shape}\")\n",
        "print(f\"지정 구간 오디오 임베딩 크기: {segment_audio_emb.shape}\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChHam3TP3yzo",
        "outputId": "bc88741b-ae0f-4eaf-d08d-b27fcaa6d0c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbaWK8QZ3wbj",
        "outputId": "ff07f2c2-f4cd-42b0-c45f-c0c67227f3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "통합 비디오-오디오 인코더 초기화 중...\n",
            "비디오 인코더 장치 사용: cuda\n",
            "BEiT3 가중치 로드 - Missing keys: 2, Unexpected keys: 3\n",
            "BEiT3 (base) 임베딩 모델 초기화 완료\n",
            "오디오 인코더 장치 사용: cuda\n",
            "통합 인코더 초기화 완료\n"
          ]
        }
      ],
      "source": [
        "# 1. 인코더 초기화 (한 번만 실행)\n",
        "encoder = VideoAudioEncoder(\n",
        "    beit3_checkpoint_path=\"/content/drive/MyDrive/sentiment_data/phr/beit3_base_patch16_384_coco_retrieval.pth\",\n",
        "    beit3_model_type='base',\n",
        "    input_size=384\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bhdpIO45RTL",
        "outputId": "9ef7a854-61dc-403a-f75c-e8e9131de566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "세그먼트 0~5초 처리 중...\n",
            "세그먼트 5~10초 처리 중...\n",
            "세그먼트 10~15초 처리 중...\n",
            "세그먼트 15~20초 처리 중...\n",
            "세그먼트 20~25초 처리 중...\n",
            "세그먼트 25~30초 처리 중...\n",
            "세그먼트 30~35초 처리 중...\n",
            "세그먼트 35~40초 처리 중...\n",
            "세그먼트 40~45초 처리 중...\n",
            "세그먼트 45~50초 처리 중...\n",
            "세그먼트 50~55초 처리 중...\n",
            "세그먼트 55~60초 처리 중...\n",
            "세그먼트 60~65초 처리 중...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/sentiment_data/phr/TOO BAD.mp4, 2764800 bytes wanted but 0 bytes read,at frame 1630/1651, at time 67.98/68.82 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "세그먼트 65~68.82초 처리 중...\n",
            "세그먼트 수: 14\n"
          ]
        }
      ],
      "source": [
        "# 3. 비디오를 세그먼트로 나누어 임베딩 추출\n",
        "video_embs, audio_embs = encoder.encode_video_by_segments('/content/drive/MyDrive/sentiment_data/phr/TOO BAD.mp4', segment_duration=5, fps=1)\n",
        "print(f\"세그먼트 수: {len(video_embs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjwLkQlc5f18",
        "outputId": "f43ff242-3700-4645-dd90-059e931cef3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "segment 0\n",
            "tensor([-0.2494, -0.6885,  0.1377,  0.4764, -0.1825])\n",
            "tensor([ 0.0623,  0.0137, -0.0132, -0.0499, -0.1548])\n",
            "segment 1\n",
            "tensor([-1.2907, -0.0029,  0.5133,  0.7455, -0.1321])\n",
            "tensor([-0.0035,  0.0667, -0.0173,  0.0430, -0.0956])\n",
            "segment 2\n",
            "tensor([-1.1876, -0.7312,  0.7007,  0.5254, -0.7939])\n",
            "tensor([ 0.0145,  0.0032, -0.0426,  0.0218, -0.0253])\n",
            "segment 3\n",
            "tensor([-0.9164,  0.0519,  0.5960,  0.9317, -0.5235])\n",
            "tensor([ 0.0156,  0.0154, -0.0573,  0.0045, -0.0125])\n",
            "segment 4\n",
            "tensor([ 0.2548,  0.0559,  0.2362,  0.0281, -0.4912])\n",
            "tensor([ 0.0039,  0.0250, -0.0551,  0.0162, -0.0052])\n",
            "segment 5\n",
            "tensor([-0.4852, -0.0937,  0.2378,  0.0432,  0.0352])\n",
            "tensor([-0.0101, -0.0216, -0.0126,  0.0566, -0.0318])\n",
            "segment 6\n",
            "tensor([ 0.2970, -0.1300,  0.3104,  0.1456, -0.6024])\n",
            "tensor([-0.0572,  0.0638, -0.0412, -0.0182,  0.0071])\n",
            "segment 7\n",
            "tensor([-0.2529, -1.7990,  0.3514,  0.0387, -1.0951])\n",
            "tensor([ 0.0294, -0.0698, -0.0288, -0.0650, -0.0505])\n",
            "segment 8\n",
            "tensor([-0.3489, -1.0902,  0.0550,  0.5993, -1.0797])\n",
            "tensor([ 0.0475,  0.0352, -0.0333,  0.0090, -0.0858])\n",
            "segment 9\n",
            "tensor([-0.1185, -1.3844,  0.1601,  0.4911, -0.6826])\n",
            "tensor([ 0.0519, -0.0339, -0.0599,  0.0229, -0.0618])\n",
            "segment 10\n",
            "tensor([-0.2077, -0.2040,  0.0559,  0.2477, -0.9103])\n",
            "tensor([ 0.0239, -0.0399, -0.0427,  0.0219, -0.0458])\n",
            "segment 11\n",
            "tensor([ 0.4524,  2.0697,  0.6027,  0.1931, -0.9023])\n",
            "tensor([-0.0258,  0.0839, -0.0505,  0.0054, -0.0140])\n",
            "segment 12\n",
            "tensor([-0.1196,  1.1253,  0.6121,  0.0646, -0.6303])\n",
            "tensor([-0.0222,  0.0227, -0.0050,  0.0113, -0.0260])\n",
            "segment 13\n",
            "tensor([ 0.1440,  1.8065,  0.2679, -0.3702, -0.5763])\n",
            "tensor([-0.0066, -0.0062,  0.0033,  0.0275, -0.0441])\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(video_embs)):\n",
        "  print(\"segment\", i)\n",
        "  print(video_embs[i][:5])\n",
        "  print(audio_embs[i][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riub6fzm5jSg"
      },
      "source": [
        "# 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 경로 설정\n",
        "beit3_checkpoint = \"path/to/beit3_base_patch16_224.pth\"\n",
        "video_path = \"path/to/video.mp4\"\n",
        "\n",
        "## 1. 인코더 초기화\n",
        "encoder = VideoAudioEncoder(\n",
        "    beit3_checkpoint_path=beit3_checkpoint,\n",
        "    beit3_model_type='base',\n",
        "    hubert_model_name='facebook/hubert-base-ls960',\n",
        "    input_size=224  # 224 또는 384 (체크포인트에 맞게 설정)\n",
        ")\n",
        "\n",
        "## 2. 전체 비디오의 단일 임베딩 추출\n",
        "video_emb, audio_emb = encoder.encode_video_segment(video_path)\n",
        "print(f\"전체 비디오 임베딩 크기: {video_emb.shape}\")\n",
        "print(f\"전체 오디오 임베딩 크기: {audio_emb.shape}\")\n",
        "\n",
        "## 3. 비디오를 세그먼트로 나누어 임베딩 추출\n",
        "video_embs, audio_embs = encoder.encode_video_by_segments(video_path, segment_duration=5, fps=1)\n",
        "print(f\"세그먼트 수: {len(video_embs)}\")\n",
        "\n",
        "## 4. 세그먼트 임베딩을 텐서로 변환\n",
        "video_embs_tensor = torch.stack(video_embs)\n",
        "audio_embs_tensor = torch.stack(audio_embs)\n",
        "print(f\"비디오 임베딩 텐서 크기: {video_embs_tensor.shape}\")\n",
        "print(f\"오디오 임베딩 텐서 크기: {audio_embs_tensor.shape}\")\n",
        "\n",
        "## 5. 특정 구간만 임베딩 추출\n",
        "segment_video_emb, segment_audio_emb = encoder.encode_video_segment(\n",
        "    video_path,\n",
        "    start_time=10,\n",
        "    end_time=15,\n",
        "    fps=1\n",
        ")\n",
        "\n",
        "print(f\"지정 구간 비디오 임베딩 크기: {segment_video_emb.shape}\")\n",
        "print(f\"지정 구간 오디오 임베딩 크기: {segment_audio_emb.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
